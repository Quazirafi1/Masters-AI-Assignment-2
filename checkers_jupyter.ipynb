{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python.exe -m pip install --upgrade pip\n",
    "# !pip install imparaai-checkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checkers.game import Game\n",
    "import copy\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import time\n",
    "import numpy as np  # Import NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Heuristics:\n",
    "    @staticmethod\n",
    "    def get_player_pieces(game, player):\n",
    "        player_pieces = []\n",
    "        for piece in game.board.pieces:\n",
    "            if piece.player == player and not piece.captured:\n",
    "                player_pieces.append(piece)\n",
    "        return player_pieces\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_opponent_pieces(game, player):\n",
    "        opponent_pieces = []\n",
    "        for piece in game.board.pieces:\n",
    "            if piece.player != player and not piece.captured:\n",
    "                opponent_pieces.append(piece)\n",
    "        return opponent_pieces\n",
    "    \n",
    "    #heuristics list starts here\n",
    "    #increase the number of pieces of the player as much as possible, maximize the difference between the player and opponent\n",
    "    @staticmethod\n",
    "    def simple_piece_count(game, player):\n",
    "        player_pieces = Heuristics.get_player_pieces(game, player)\n",
    "        opponent_pieces = Heuristics.get_opponent_pieces(game, player)\n",
    "        \n",
    "        return len(player_pieces) - len(opponent_pieces)\n",
    "    \n",
    "    #try to increase the number of king pieces the player has\n",
    "    @staticmethod\n",
    "    def king_count(game, player):\n",
    "        player_pieces = Heuristics.get_player_pieces(game, player)\n",
    "        return sum(1 for piece in player_pieces if piece.king)\n",
    "    \n",
    "    #try to have as much as dominance possible in the centre board\n",
    "    @staticmethod\n",
    "    def evaluate_board_control(game, player):\n",
    "        #Higher score for central squares, Medium score for middle squares, lower score for back rows\n",
    "        board_control_scores = {\n",
    "            (1, 2, 3, 4, 5, 6, 7, 8): 5,  \n",
    "            (9, 10, 11, 12, 21, 22, 23, 24): 3,  \n",
    "            (13, 14, 15, 16, 17, 18, 19, 20): 1, \n",
    "        }\n",
    "\n",
    "        total_score = 0\n",
    "        # Use a set to collect unique positions\n",
    "        total_positions = set()  \n",
    "        \n",
    "        pieces = Heuristics.get_player_pieces(game, player)\n",
    "        \n",
    "        for piece in pieces:\n",
    "            for positions, score in board_control_scores.items():\n",
    "                if piece.position in positions:\n",
    "                    total_score += score\n",
    "                    # Add the positions in the group to the set\n",
    "                    total_positions.update(positions)\n",
    "                    # Assign the highest available score  \n",
    "                    break  \n",
    "\n",
    "        return total_score\n",
    "    #heuristics list ends here    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimaxAgent(Heuristics):\n",
    "    def __init__(self, player):\n",
    "        self.player = player\n",
    "        self.regressor = None\n",
    "    \n",
    "    def handle_empty_data(self, game, heuristic, observations):\n",
    "        valid_moves = game.get_possible_moves()\n",
    "        # Implement your logic for handling empty data here\n",
    "        if not valid_moves:\n",
    "            return None, False\n",
    "        return random.choice(valid_moves), True\n",
    "\n",
    "    def get_best_move(self, game, heuristic, observations, data, stage):\n",
    "        valid_moves = game.get_possible_moves()  # Get a list of valid moves\n",
    "\n",
    "        if stage == \"RF\":\n",
    "            # Try to use the regressor if it's available\n",
    "            if self.regressor:\n",
    "                _, best_move = self.minimax(game, 3, True, float('-inf'), float('inf'), heuristic, 2, observations)\n",
    "                if best_move in valid_moves:\n",
    "                    return best_move, False\n",
    "                else:\n",
    "                    return random.choice(valid_moves), True\n",
    "\n",
    "        _, best_move = self.minimax(game, 3, True, float('-inf'), float('inf'), heuristic, 2, observations, stage)\n",
    "        \n",
    "        # Ensure the best move is a valid move\n",
    "        if best_move in valid_moves:\n",
    "            return best_move, False\n",
    "        if stage == \"RF\" or stage == \"Hl\":\n",
    "            # If all else fails, handle empty data\n",
    "            move, flag = self.handle_empty_data(game, heuristic, observations)\n",
    "            return move, flag\n",
    "\n",
    "\n",
    "\n",
    "    def minimax(self, game, depth, maximizing_player, alpha, beta, heuristic, l, observations, stage):\n",
    "        #selection of the heuristics for position evaluation when either the game is over or at the leaf nodes\n",
    "        if depth == 0 or game.is_over():\n",
    "            if stage == \"H0\" or stage == \"alpha-beta\":\n",
    "                return self.evaluate_H0(game, heuristic), None\n",
    "            elif stage == \"Hl\":\n",
    "                return self.evaluate_Hl(game, heuristic, l), None\n",
    "            elif stage == \"RF\":\n",
    "                if self.regressor:\n",
    "                    predicted_HL = self.regressor.predict([list(observations.values())])\n",
    "                    return predicted_HL[0], None\n",
    "                else:\n",
    "                    # If the regressor is not trained, fall back to H0\n",
    "                    return self.evaluate_H0(game, heuristic), None\n",
    "            \n",
    "        if maximizing_player:\n",
    "            # Initialize the maximum evaluation.\n",
    "            max_eval = float('-inf')\n",
    "            # Initialize the best move.\n",
    "            best_move = None\n",
    "            \n",
    "            # Loop through possible moves in the game.\n",
    "            for move in game.get_possible_moves():\n",
    "                # Create a copy of the game for simulation.\n",
    "                next_game = copy.deepcopy(game)\n",
    "                # Apply the current move to the copy of the game.\n",
    "                next_game.move(move)\n",
    "\n",
    "                if stage == \"H0\":\n",
    "                    eval = self.evaluate_H0(next_game, heuristic)\n",
    "                elif stage == \"Hl\" or stage == \"RF\" or stage == \"alpha-beta\":\n",
    "                    eval, _ = self.minimax(next_game, depth - 1, False, alpha, beta, heuristic, l, observations, stage)\n",
    "                \n",
    "    \n",
    "                # If the evaluation is better than the current maximum: update the maximum evaluationand best move\n",
    "                if eval > max_eval:\n",
    "                    max_eval = eval\n",
    "                    best_move = move\n",
    "\n",
    "                # Update the alpha value.\n",
    "                alpha = max(alpha, max_eval)\n",
    "                # Prune the search if the beta value is less than or equal to alpha.\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "\n",
    "            return max_eval, best_move\n",
    "        else:\n",
    "            # Initialization of the minimizing player\n",
    "            min_eval = float('inf')\n",
    "            best_move = None\n",
    "\n",
    "            # Loop through possible moves in the game.    \n",
    "            for move in game.get_possible_moves():\n",
    "                # Create a copy of the game for simulation.\n",
    "                next_game = copy.deepcopy(game)\n",
    "                next_game.move(move)\n",
    "\n",
    "                if stage == \"H0\":\n",
    "                    eval = self.evaluate_H0(next_game, heuristic)\n",
    "                elif stage == \"Hl\" or stage == \"RF\" or stage == \"alpha-beta\":\n",
    "                    eval, _ = self.minimax(next_game, depth - 1, True, alpha, beta, heuristic, l, observations, stage)\n",
    "                \n",
    "                # If the evaluation is worse than the current minimum: update the minimum evaluation and best move\n",
    "                if eval < min_eval:\n",
    "                    min_eval = eval\n",
    "                    best_move = move    \n",
    "\n",
    "                # Update the alpha value.\n",
    "                beta = min(beta, min_eval)\n",
    "                # Prune the search if the beta value is less than or equal to alpha.\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "\n",
    "            return min_eval, best_move\n",
    "        \n",
    "    @staticmethod\n",
    "    def preprocess_data(data):\n",
    "        X = [list(observation.values()) for observation, hl_value in data]\n",
    "        y = [hl_value for observation, hl_value in data]\n",
    "        return X, y\n",
    "\n",
    "    def train_regressor(self, data):\n",
    "        X, y = MinimaxAgent.preprocess_data(data)\n",
    "        print(\"Number of samples in X before filtering:\", len(X))\n",
    "        print(\"Number of samples in y before filtering:\", len(y))\n",
    "\n",
    "        # Filter out samples with non-finite y values and corresponding X values\n",
    "        finite_indices = [i for i, value in enumerate(y) if np.isfinite(value)]\n",
    "        if len(finite_indices) != len(y):\n",
    "            print(f\"Removing {len(y) - len(finite_indices)} samples with non-finite y values.\")\n",
    "\n",
    "        X = [X[i] for i in finite_indices]\n",
    "        y = [y[i] for i in finite_indices]\n",
    "\n",
    "        print(\"Number of samples in X after filtering:\", len(X))\n",
    "        print(\"Number of samples in y after filtering:\", len(y))\n",
    "\n",
    "        # Ensure that X and y have the same number of samples\n",
    "        if len(X) != len(y):\n",
    "            raise ValueError(\"Number of samples in X and y do not match\")\n",
    "\n",
    "        regressor = RandomForestRegressor()\n",
    "        if len(X) > 0:\n",
    "            regressor.fit(X, y)\n",
    "        self.regressor = regressor\n",
    "    \n",
    "    \n",
    "    # Add a method to calculate H0 evaluation for a given game state\n",
    "    def evaluate_H0(self, game, heuristic):\n",
    "        if heuristic == \"piece_count\":\n",
    "            return Heuristics.simple_piece_count(game, self.player)\n",
    "        elif heuristic == \"king_count\":\n",
    "            return Heuristics.king_count(game, self.player)\n",
    "        else:\n",
    "            return Heuristics.evaluate_board_control(game, self.player)\n",
    "        \n",
    "    def evaluate_Hl(self, game, heuristic, l):\n",
    "        if l == 0:\n",
    "            return self.evaluate_H0(game, heuristic)\n",
    "\n",
    "        def depth_l_evaluation(game, current_depth):\n",
    "            if current_depth == l:\n",
    "                return self.evaluate_H0(game, heuristic)\n",
    "\n",
    "            maximizing_player = (current_depth % 2 == 0)\n",
    "\n",
    "            if maximizing_player:\n",
    "                max_eval = float('-inf')\n",
    "                for move in game.get_possible_moves():\n",
    "                    next_game = copy.deepcopy(game)\n",
    "                    if next_game.move(move):\n",
    "                        eval = depth_l_evaluation(next_game, current_depth + 1)\n",
    "                        max_eval = max(max_eval, eval)\n",
    "                return max_eval\n",
    "            else:\n",
    "                min_eval = float('inf')\n",
    "                for move in game.get_possible_moves():\n",
    "                    next_game = copy.deepcopy(game)\n",
    "                    if next_game.move(move):\n",
    "                        eval = depth_l_evaluation(next_game, current_depth + 1)\n",
    "                        min_eval = min(min_eval, eval)\n",
    "                return min_eval\n",
    "\n",
    "        # Initialize the evaluation to be computed at level l.\n",
    "        return depth_l_evaluation(game, current_depth=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing piece_count for maximizing agents for iteration-0 and alpha-beta evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-0 and H0 evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-0 and Hl evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-0 and RF evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-1 and alpha-beta evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-1 and H0 evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-1 and Hl evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-1 and RF evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-2 and alpha-beta evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-2 and H0 evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-2 and Hl evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-2 and RF evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-3 and alpha-beta evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-3 and H0 evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-3 and Hl evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-3 and RF evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-4 and alpha-beta evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-4 and H0 evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-4 and Hl evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-4 and RF evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-5 and alpha-beta evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-5 and H0 evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-5 and Hl evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-5 and RF evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-6 and alpha-beta evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-6 and H0 evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-6 and Hl evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-6 and RF evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-7 and alpha-beta evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-7 and H0 evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-7 and Hl evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-7 and RF evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-8 and alpha-beta evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-8 and H0 evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-8 and Hl evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-8 and RF evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-9 and alpha-beta evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-9 and H0 evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-9 and Hl evaluation\n",
      "Comparing piece_count for maximizing agents for iteration-9 and RF evaluation\n",
      "Number of samples in X before filtering: 303\n",
      "Number of samples in y before filtering: 303\n",
      "Removing 6 samples with non-finite y values.\n",
      "Number of samples in X after filtering: 297\n",
      "Number of samples in y after filtering: 297\n",
      "piece_count_durations [0.9935882091522217, 0.19998955726623535, 26.195032835006714, 1.2490699291229248, 0.7201669216156006, 0.21260476112365723, 48.859835624694824, 1.1157917976379395, 0.8165438175201416, 0.09936881065368652, 24.631739377975464, 1.1137678623199463, 0.8829193115234375, 0.1535036563873291, 14.82118010520935, 3.0936052799224854, 0.7821981906890869, 0.10042071342468262, 32.55633807182312, 3.1809158325195312, 0.5995216369628906, 0.23189735412597656, 29.265795707702637, 1.6920130252838135, 0.8625950813293457, 0.14203810691833496, 28.17732071876526, 1.1610839366912842, 2.214888572692871, 0.14975690841674805, 41.782315254211426, 1.7155492305755615, 2.0019280910491943, 0.19608235359191895, 38.63452935218811, 3.0134177207946777, 0.7507467269897461, 0.11634445190429688, 16.186673402786255, 3.7137420177459717]\n"
     ]
    }
   ],
   "source": [
    "def play_game(agent, game, heuristic, data, stage):\n",
    "    current_player = game.whose_turn()\n",
    "    game.consecutive_noncapture_move_limit = 20\n",
    "    opponent_moves_count = 0\n",
    "    agent_moves_count = 0\n",
    "    observations = {}\n",
    "\n",
    "    while not game.is_over():\n",
    "        if current_player == agent.player:\n",
    "            if stage == \"RF\":\n",
    "                observations = {\n",
    "                    'num_pieces': len(agent.get_player_pieces(game, agent.player))\n",
    "                }\n",
    "                \n",
    "            best_move, random_checker = agent.get_best_move(game, heuristic, observations, data, stage)\n",
    "            # Check if the best_move is valid\n",
    "            valid_moves = game.get_possible_moves()\n",
    "            \n",
    "            if best_move in valid_moves:\n",
    "                game.move(best_move)  # Apply the move\n",
    "                agent_moves_count += 1\n",
    "                \n",
    "                if stage == \"RF\":\n",
    "                    # Collect high-level value for this move (hl_value)\n",
    "                    hl_value = agent.evaluate_Hl(game, heuristic, 2)\n",
    "\n",
    "                    # Append observations and hl_value as a tuple to the data list\n",
    "                    data.append((observations, hl_value))\n",
    "                # print(f\"Your agent's move: {best_move}\")\n",
    "                \n",
    "                if not random_checker:\n",
    "                    1\n",
    "                    # print(f\"Your agent's move: {best_move}\")\n",
    "                else:\n",
    "                    2\n",
    "                    # print(f\"Your agent's random move: {best_move} + {random_checker}\")\n",
    "                    \n",
    "        else:\n",
    "            #the opponent plays randomly for baseline comparison\n",
    "            opponent_moves = game.get_possible_moves()\n",
    "            opponent_move = random.choice(opponent_moves)\n",
    "            game.move(opponent_move)\n",
    "            opponent_moves_count += 1\n",
    "            # print(f\"Opponent's move: {opponent_move}\")\n",
    "            \n",
    "        # Switch the current player for the next turn\n",
    "        if not game.move_limit_reached():\n",
    "            current_player = game.whose_turn()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    piece_count_agent_moves.append(agent_moves_count)\n",
    "    # print(f\"Total moves by your agent: {agent_moves_count}\")\n",
    "    # print(f\"Total moves by the opponent: {opponent_moves_count}\")\n",
    "    winner = game.get_winner()\n",
    "\n",
    "    if winner == 1:\n",
    "        # print(\"The agent won!\")\n",
    "        return 1\n",
    "    elif winner == 2:\n",
    "        # print(\"The opponent won!\")\n",
    "        return -1\n",
    "    else:\n",
    "        # print(\"It's a draw!\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "piece_count_wins = []\n",
    "piece_count_losses = []\n",
    "piece_count_draws = []\n",
    "piece_count_durations = []\n",
    "piece_count_agent_moves = []\n",
    "data = []\n",
    "stages = [\"alpha-beta\" ,\"H0\", \"Hl\", \"RF\"]\n",
    "# stages = [\"Hl\"]\n",
    "for i in range(10):\n",
    "    for stage in stages:\n",
    "        heuristic = 'piece_count'\n",
    "        print(f\"Comparing {heuristic} for maximizing agents for iteration-{i} and {stage} evaluation\")\n",
    "        start_time = time.time()\n",
    "        game = Game()\n",
    "        agent = MinimaxAgent(1)\n",
    "        result = play_game(agent, game, heuristic, data, stage)\n",
    "        piece_count_durations.append(time.time() - start_time)\n",
    "\n",
    "        if result == 1:\n",
    "            piece_count_wins.append(1)\n",
    "            piece_count_losses.append(0)\n",
    "            piece_count_draws.append(0)\n",
    "        elif result == -1:\n",
    "            piece_count_wins.append(0)\n",
    "            piece_count_losses.append(1)\n",
    "            piece_count_draws.append(0)\n",
    "        else:\n",
    "            piece_count_wins.append(0)\n",
    "            piece_count_losses.append(0)\n",
    "            piece_count_draws.append(1)\n",
    "\n",
    "# Train the regressor using the collected data\n",
    "if stage == \"RF\":\n",
    "    agent.train_regressor(data)\n",
    "\n",
    "# print(f\"piece_count_wins {piece_count_wins}\")\n",
    "# print(f\"piece_count_losses {piece_count_losses}\")\n",
    "# print(f\"piece_count_draws {piece_count_draws}\")\n",
    "print(f\"piece_count_durations {piece_count_durations}\")\n",
    "# print(f\"piece_count_moves {piece_count_agent_moves}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from checkers.game import Game\n",
    "# import copy\n",
    "# import random\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# import time\n",
    "# import numpy as np  # Import NumPy\n",
    "\n",
    "\n",
    "# class Heuristics:\n",
    "#     @staticmethod\n",
    "#     def get_player_pieces(game, player):\n",
    "#         player_pieces = []\n",
    "#         for piece in game.board.pieces:\n",
    "#             if piece.player == player and not piece.captured:\n",
    "#                 player_pieces.append(piece)\n",
    "#         return player_pieces\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def get_opponent_pieces(game, player):\n",
    "#         opponent_pieces = []\n",
    "#         for piece in game.board.pieces:\n",
    "#             if piece.player != player and not piece.captured:\n",
    "#                 opponent_pieces.append(piece)\n",
    "#         return opponent_pieces\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def simple_piece_count(game, player):\n",
    "#         player_pieces = Heuristics.get_player_pieces(game, player)\n",
    "#         opponent_pieces = Heuristics.get_opponent_pieces(game, player)\n",
    "        \n",
    "#         return len(player_pieces) - len(opponent_pieces)\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def king_count(game, player):\n",
    "#         player_pieces = Heuristics.get_player_pieces(game, player)\n",
    "#         return sum(1 for piece in player_pieces if piece.king)\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def evaluate_board_control(game, player):\n",
    "#         board_control_scores = {\n",
    "#             (1, 2, 3, 4, 5, 6, 7, 8): 5,  \n",
    "#             (9, 10, 11, 12, 21, 22, 23, 24): 3,  \n",
    "#             (13, 14, 15, 16, 17, 18, 19, 20): 1, \n",
    "#         }\n",
    "\n",
    "#         total_score = 0\n",
    "#         total_positions = set()  \n",
    "        \n",
    "#         pieces = Heuristics.get_player_pieces(game, player)\n",
    "        \n",
    "#         for piece in pieces:\n",
    "#             for positions, score in board_control_scores.items():\n",
    "#                 if piece.position in positions:\n",
    "#                     total_score += score\n",
    "#                     total_positions.update(positions)\n",
    "#                     break  \n",
    "\n",
    "#         return total_score\n",
    "\n",
    "# class MinimaxAgent(Heuristics):\n",
    "#     def __init__(self, player):\n",
    "#         self.player = player\n",
    "#         self.regressor = None\n",
    "\n",
    "#     def handle_empty_data(self, game, heuristic, observations):\n",
    "#         valid_moves = game.get_possible_moves()\n",
    "#         # Implement your logic for handling empty data here\n",
    "#         if not valid_moves:\n",
    "#             return None, False\n",
    "#         return random.choice(valid_moves), True\n",
    "\n",
    "#     def get_best_move(self, game, heuristic, observations, data):\n",
    "#         valid_moves = game.get_possible_moves()\n",
    "#         if not valid_moves:\n",
    "#             return None, False\n",
    "\n",
    "#         # Try to use the regressor if it's available\n",
    "#         if self.regressor:\n",
    "#             _, best_move = self.minimax(game, 3, True, float('-inf'), float('inf'), heuristic, 2, observations)\n",
    "#             if best_move in valid_moves:\n",
    "#                 return best_move, False\n",
    "#             else:\n",
    "#                 return random.choice(valid_moves), True\n",
    "\n",
    "#         # If no regressor, then try to use the minimax logic to get a move\n",
    "#         _, best_move = self.minimax(game, 3, True, float('-inf'), float('inf'), heuristic, 2, observations)\n",
    "#         if best_move in valid_moves:\n",
    "#             return best_move, False\n",
    "\n",
    "#         # If all else fails, handle empty data\n",
    "#         move, flag = self.handle_empty_data(game, heuristic, observations)\n",
    "#         return move, flag\n",
    "\n",
    "\n",
    "#     def minimax(self, game, depth, maximizing_player, alpha, beta, heuristic, l, observations):\n",
    "#         if depth == 0 or game.is_over():\n",
    "#             if self.regressor:\n",
    "#                 predicted_HL = self.regressor.predict([list(observations.values())])\n",
    "#                 return predicted_HL[0], None\n",
    "#             else:\n",
    "#                 # If the regressor is not trained, fall back to H0\n",
    "#                 return self.evaluate_H0(game), None\n",
    "\n",
    "\n",
    "#         if maximizing_player:\n",
    "#             max_eval = float('-inf')\n",
    "#             best_move = None\n",
    "\n",
    "#             for move in game.get_possible_moves():\n",
    "#                 next_game = copy.deepcopy(game)\n",
    "#                 next_game.move(move)\n",
    "#                 eval, _ = self.minimax(next_game, depth - 1, False, alpha, beta, heuristic, l, observations)\n",
    "\n",
    "#                 if eval > max_eval:\n",
    "#                     max_eval = eval\n",
    "#                     best_move = move\n",
    "\n",
    "#                 alpha = max(alpha, max_eval)\n",
    "#                 if beta <= alpha:\n",
    "#                     break\n",
    "\n",
    "#             return max_eval, best_move\n",
    "#         else:\n",
    "#             min_eval = float('inf')\n",
    "#             best_move = None\n",
    "\n",
    "#             for move in game.get_possible_moves():\n",
    "#                 next_game = copy.deepcopy(game)\n",
    "#                 next_game.move(move)\n",
    "#                 eval, _ = self.minimax(next_game, depth - 1, True, alpha, beta, heuristic, l, observations)\n",
    "\n",
    "#                 if eval < min_eval:\n",
    "#                     min_eval = eval\n",
    "#                     best_move = move\n",
    "\n",
    "#                 beta = min(beta, min_eval)\n",
    "#                 if beta <= alpha:\n",
    "#                     break\n",
    "\n",
    "#             return min_eval, best_move\n",
    "\n",
    "#     @staticmethod\n",
    "#     def preprocess_data(data):\n",
    "#         X = [list(observation.values()) for observation, hl_value in data]\n",
    "#         y = [hl_value for observation, hl_value in data]\n",
    "#         return X, y\n",
    "\n",
    "#     def train_regressor(self, data):\n",
    "#         X, y = MinimaxAgent.preprocess_data(data)\n",
    "#         print(\"Number of samples in X before filtering:\", len(X))\n",
    "#         print(\"Number of samples in y before filtering:\", len(y))\n",
    "\n",
    "#         # Filter out samples with non-finite y values and corresponding X values\n",
    "#         finite_indices = [i for i, value in enumerate(y) if np.isfinite(value)]\n",
    "#         if len(finite_indices) != len(y):\n",
    "#             print(f\"Removing {len(y) - len(finite_indices)} samples with non-finite y values.\")\n",
    "\n",
    "#         X = [X[i] for i in finite_indices]\n",
    "#         y = [y[i] for i in finite_indices]\n",
    "\n",
    "#         print(\"Number of samples in X after filtering:\", len(X))\n",
    "#         print(\"Number of samples in y after filtering:\", len(y))\n",
    "\n",
    "#         # Ensure that X and y have the same number of samples\n",
    "#         if len(X) != len(y):\n",
    "#             raise ValueError(\"Number of samples in X and y do not match\")\n",
    "\n",
    "#         regressor = RandomForestRegressor()\n",
    "#         if len(X) > 0:\n",
    "#             regressor.fit(X, y)\n",
    "#         self.regressor = regressor\n",
    "\n",
    "\n",
    "#     def evaluate_H0(self, game):\n",
    "#         # Getting multiple observations\n",
    "#         piece_count = Heuristics.simple_piece_count(game, self.player)\n",
    "#         # king_count = Heuristics.king_count(game, self.player)\n",
    "#         # board_control = Heuristics.evaluate_board_control(game, self.player)\n",
    "\n",
    "#         # You can combine these observations into a composite score in various ways. \n",
    "#         # Here, I'm just combining them linearly as an example.\n",
    "#         # In practice, you might want to give different weights or use a trained model.\n",
    "#         # return piece_count + king_count + board_control\n",
    "#         return piece_count\n",
    "\n",
    "\n",
    "#     def evaluate_Hl(self, game, heuristic, l):\n",
    "#         if l == 0:\n",
    "#             return self.evaluate_H0(game)\n",
    "\n",
    "#         def depth_l_evaluation(game, current_depth):\n",
    "#             if current_depth == l:\n",
    "#                 return self.evaluate_H0(game)\n",
    "\n",
    "#             maximizing_player = (current_depth % 2 == 0)\n",
    "\n",
    "#             if maximizing_player:\n",
    "#                 max_eval = float('-inf')\n",
    "#                 for move in game.get_possible_moves():\n",
    "#                     next_game = copy.deepcopy(game)\n",
    "#                     if next_game.move(move):\n",
    "#                         eval = depth_l_evaluation(next_game, current_depth + 1)\n",
    "#                         max_eval = max(max_eval, eval)\n",
    "#                 return max_eval\n",
    "#             else:\n",
    "#                 min_eval = float('inf')\n",
    "#                 for move in game.get_possible_moves():\n",
    "#                     next_game = copy.deepcopy(game)\n",
    "#                     if next_game.move(move):\n",
    "#                         eval = depth_l_evaluation(next_game, current_depth + 1)\n",
    "#                         min_eval = min(min_eval, eval)\n",
    "#                 return min_eval\n",
    "\n",
    "#         return depth_l_evaluation(game, current_depth=0)\n",
    "\n",
    "# def play_game(agent, game, heuristic, data):\n",
    "#     current_player = game.whose_turn()\n",
    "#     game.consecutive_noncapture_move_limit = 20\n",
    "#     opponent_moves_count = 0\n",
    "#     agent_moves_count = 0\n",
    "#     while not game.is_over():\n",
    "#         if current_player == agent.player:\n",
    "#             observations = {\n",
    "#                 'num_pieces': len(agent.get_player_pieces(game, agent.player))\n",
    "#             }\n",
    "#             valid_moves = game.get_possible_moves()\n",
    "#             best_move, random_checker = agent.get_best_move(game, heuristic, observations, data)\n",
    "\n",
    "#             if best_move in valid_moves:\n",
    "#                 game.move(best_move)\n",
    "#                 agent_moves_count += 1  # Update the agent's move count\n",
    "\n",
    "#                 # Collect high-level value for this move (hl_value)\n",
    "#                 hl_value = agent.evaluate_Hl(game, heuristic, 2)\n",
    "\n",
    "#                 # Append observations and hl_value as a tuple to the data list\n",
    "#                 data.append((observations, hl_value))\n",
    "#                 if not random_checker:\n",
    "#                     print(f\"Your agent's move: {best_move}\")\n",
    "#                 else:\n",
    "#                     print(f\"Your agent's random move: {best_move} and {random_checker}\")\n",
    "#             else:\n",
    "#                 print(\"Invalid move selected by the agent\")\n",
    "#         else:\n",
    "#             opponent_moves = game.get_possible_moves()\n",
    "#             opponent_move = random.choice(opponent_moves)\n",
    "#             game.move(opponent_move)\n",
    "#             opponent_moves_count += 1\n",
    "#             print(f\"Opponent's move: {opponent_move}\")\n",
    "\n",
    "#         if not game.move_limit_reached():\n",
    "#             current_player = game.whose_turn()\n",
    "#         else:\n",
    "#             break\n",
    "\n",
    "#     piece_count_agent_moves.append(agent_moves_count)  # Update the agent's move count\n",
    "#     print(f\"Total moves by your agent: {agent_moves_count}\")\n",
    "#     print(f\"Total moves by the opponent: {opponent_moves_count}\")\n",
    "\n",
    "#     winner = game.get_winner()\n",
    "\n",
    "#     if winner == 1:\n",
    "#         print(\"The agent won!\")\n",
    "#         return 1\n",
    "#     elif winner == 2:\n",
    "#         print(\"The opponent won!\")\n",
    "#         return -1\n",
    "#     else:\n",
    "#         print(\"It's a draw!\")\n",
    "#         return 0\n",
    "\n",
    "\n",
    "# piece_count_wins = []\n",
    "# piece_count_losses = []\n",
    "# piece_count_draws = []\n",
    "# piece_count_durations = []\n",
    "# piece_count_agent_moves = []\n",
    "# data = []\n",
    "\n",
    "# for i in range(10):\n",
    "#     heuristic = 'piece_count'\n",
    "#     print(f\"Comparing {heuristic} for maximizing agents for iteration-{i}\")\n",
    "#     start_time = time.time()\n",
    "#     game = Game()\n",
    "#     agent = MinimaxAgent(1)\n",
    "#     result = play_game(agent, game, heuristic, data)\n",
    "#     piece_count_durations.append(time.time() - start_time)\n",
    "\n",
    "#     if result == 1:\n",
    "#         piece_count_wins.append(1)\n",
    "#         piece_count_losses.append(0)\n",
    "#         piece_count_draws.append(0)\n",
    "#     elif result == -1:\n",
    "#         piece_count_wins.append(0)\n",
    "#         piece_count_losses.append(1)\n",
    "#         piece_count_draws.append(0)\n",
    "#     else:\n",
    "#         piece_count_wins.append(0)\n",
    "#         piece_count_losses.append(0)\n",
    "#         piece_count_draws.append(1)\n",
    "\n",
    "# # Train the regressor using the collected data\n",
    "# agent.train_regressor(data)\n",
    "\n",
    "# print(f\"piece_count_wins {piece_count_wins}\")\n",
    "# print(f\"piece_count_losses {piece_count_losses}\")\n",
    "# print(f\"piece_count_draws {piece_count_draws}\")\n",
    "# print(f\"piece_count_durations {piece_count_durations}\")\n",
    "# print(f\"piece_count_moves {piece_count_agent_moves}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def game_outcome(wins, losses, draws, heuristic_name, iterations_per_heuristic):\n",
    "#     iterations = []\n",
    "\n",
    "#     for i in range(iterations_per_heuristic):\n",
    "#         iterations.append(\"Iteration-\" + str(i))\n",
    "    \n",
    "#     win_percentage = [win / iterations_per_heuristic * 100 for win in wins]\n",
    "#     lose_percentage = [loss / iterations_per_heuristic * 100 for loss in losses]\n",
    "#     draw_percentage = [draw / iterations_per_heuristic * 100 for draw in draws]\n",
    "    \n",
    "#     # Create an array of the x positions\n",
    "#     x = np.arange(len(iterations))\n",
    "#     bar_width = 0.25\n",
    "    \n",
    "#     # Plot the results as a grouped bar chart\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.bar(x - bar_width, win_percentage, bar_width, label='Win', color='green', alpha=0.6)\n",
    "#     plt.bar(x, lose_percentage, bar_width, label='Lose', color='red', alpha=0.6)\n",
    "#     plt.bar(x + bar_width, draw_percentage, bar_width, label='Draw', color='blue', alpha=0.6)\n",
    "\n",
    "#     plt.xlabel('Iterations')\n",
    "#     plt.ylabel('Percentage')\n",
    "#     plt.title('Game Outcomes Analysis for ' + heuristic_name)\n",
    "#     plt.xticks(x, iterations)\n",
    "#     plt.legend(loc='upper right')\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_outcome(piece_count_wins, piece_count_losses, piece_count_draws, \"Piece Count\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_outcome(king_count_wins, king_count_losses, king_count_draws, \"King Count\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_outcome(board_control_wins, board_control_losses, board_control_draws, \"Board Control\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# piece_count_average_duration = sum(piece_count_durations) / len(piece_count_durations)\n",
    "# king_count_average_duration = sum(king_count_durations) / len(king_count_durations)\n",
    "# board_control_average_duration = sum(board_control_durations) / len(board_control_durations)\n",
    "\n",
    "# average_durations = [piece_count_average_duration, king_count_average_duration, board_control_average_duration]\n",
    "\n",
    "# heuristics_list = ['Piece Count', 'King Count', 'Board Control']\n",
    "\n",
    "# # Create a bar chart\n",
    "# plt.bar(heuristics_list, average_durations, color=['red', 'green', 'blue'])\n",
    "# plt.xlabel('Heuristic')\n",
    "# plt.ylabel('Average Durations (seconds)')\n",
    "# plt.title('Average Game Duration Analysis')\n",
    "\n",
    "# # Display the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# piece_count_average_agent_moves = sum(piece_count_agent_moves) / len(piece_count_agent_moves)\n",
    "# king_count_average_agent_moves = sum(king_count_agent_moves) / len(king_count_agent_moves)\n",
    "# board_control_average_agent_moves = sum(board_control_agent_moves) / len(board_control_agent_moves)\n",
    "\n",
    "# average_agent_moves = [piece_count_average_agent_moves, king_count_average_agent_moves, board_control_average_agent_moves]\n",
    "\n",
    "# # heuristics_list = ['Piece Count', 'King Count', 'Board Control']\n",
    "\n",
    "# # Create a bar chart\n",
    "# plt.bar(heuristics_list, average_agent_moves, color=['red', 'green', 'blue'])\n",
    "# plt.xlabel('Heuristics')\n",
    "# plt.ylabel('Average Agent Moves')\n",
    "# plt.title('Average Agent Moves Analysis')\n",
    "\n",
    "# # Display the plot\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
